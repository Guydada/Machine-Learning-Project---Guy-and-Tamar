{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9382182",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import strptime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45a4c2cf",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Supplementary functions for data parsing and tidying\n",
    "\n",
    "# Month to int function\n",
    "\n",
    "def month_str_to_int(month): # convert a month's name to a float number\n",
    "    try:\n",
    "        month_num = strptime(month, '%B').tm_mon\n",
    "    except:\n",
    "        return None\n",
    "    return month_num\n",
    "\n",
    "def week_str_to_int(week): # convert a month's name to a float number\n",
    "    try:\n",
    "        week_num = week.strip(\"week_\")\n",
    "    except:\n",
    "        return None\n",
    "    return week_num\n",
    "\n",
    "\n",
    "def convert_column_to_num_month(df, column_name): #apply month_str_to_int to a whole column\n",
    "    df = df[column_name]\n",
    "    df = df.apply(month_str_to_int)\n",
    "    df = df.apply(lambda x: \"{:.0f}\".\n",
    "                  format(x) if not pd.isnull(x) else x) # format as int\n",
    "    df = df.apply(lambda x: int(x) if not pd.isnull(x) else x)\n",
    "    return df\n",
    "\n",
    "def strip_week_column(df, column_name):\n",
    "    df = df.apply(week_str_to_int)\n",
    "    df = df.apply(lambda x: \"{:.0f}\".\n",
    "                  format(x) if not pd.isnull(x) else x) # format as int\n",
    "    df = df.apply(lambda x: int(x) if not pd.isnull(x) else x)\n",
    "    return df\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def csv_load(file): # suuplementry to load a CSV file and return as df, in the future to be extended\n",
    "    df = pd.read_csv(file)\n",
    "    return df\n",
    "\n",
    "#Correlation creator\n",
    "## This function shall be used for numeric features only\n",
    "\n",
    "def plot_correlations(feature1, feature2):\n",
    "    plt.plot(df[feature1], df[feature2])\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.title((\"The correlation between {} and {}\".format(feature1, feature2)))\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_scatter(feature1, feature2):\n",
    "    plt.scatter(df[feature1], df[feature2])\n",
    "    plt.grid()\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.title((\"The correlation between {} and {}\".format(feature1, feature2)))\n",
    "    plt.show()\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31ba01d8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[20. 41. 12. ... 35. 34. 22.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-24-49ceb563f15e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[0mdf\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfillna\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[0mscaler\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mStandardScaler\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m \u001B[0mdfs\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdf\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'order_week'\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m \u001B[0mdfs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\Repo\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001B[0m in \u001B[0;36mfit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    728\u001B[0m         \u001B[1;31m# Reset internal state before fitting\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    729\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 730\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpartial_fit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    731\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    732\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mpartial_fit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\Repo\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001B[0m in \u001B[0;36mpartial_fit\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    764\u001B[0m         \"\"\"\n\u001B[0;32m    765\u001B[0m         \u001B[0mfirst_call\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"n_samples_seen_\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 766\u001B[1;33m         X = self._validate_data(X, accept_sparse=('csr', 'csc'),\n\u001B[0m\u001B[0;32m    767\u001B[0m                                 \u001B[0mestimator\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mFLOAT_DTYPES\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    768\u001B[0m                                 force_all_finite='allow-nan', reset=first_call)\n",
      "\u001B[1;32m~\\.conda\\envs\\Repo\\lib\\site-packages\\sklearn\\base.py\u001B[0m in \u001B[0;36m_validate_data\u001B[1;34m(self, X, y, reset, validate_separately, **check_params)\u001B[0m\n\u001B[0;32m    419\u001B[0m             \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    420\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0my\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;34m'no_validation'\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 421\u001B[1;33m             \u001B[0mX\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    422\u001B[0m             \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    423\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\Repo\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36minner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     61\u001B[0m             \u001B[0mextra_args\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mlen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mall_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     62\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0mextra_args\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 63\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     64\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     65\u001B[0m             \u001B[1;31m# extra_args > 0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\Repo\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001B[0m\n\u001B[0;32m    635\u001B[0m             \u001B[1;31m# If input is 1D raise error\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    636\u001B[0m             \u001B[1;32mif\u001B[0m \u001B[0marray\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndim\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 637\u001B[1;33m                 raise ValueError(\n\u001B[0m\u001B[0;32m    638\u001B[0m                     \u001B[1;34m\"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    639\u001B[0m                     \u001B[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: Expected 2D array, got 1D array instead:\narray=[20. 41. 12. ... 35. 34. 22.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# Main build\n",
    "\n",
    "df = csv_load(\"Data/feature_data.csv\") #Reading the CSV data file\n",
    "features = df.columns\n",
    "\n",
    "# Data manipulations\n",
    "\n",
    "df[\"order_month\"] = convert_column_to_num_month(df, \"order_month\")\n",
    "df['order_week'] = df['order_week'].str.strip(\"week_\")\n",
    "df['order_week'] = df['order_week'].astype(int)\n",
    "df.fillna(-1)\n",
    "scaler = StandardScaler()\n",
    "dfs = scaler.fit(df['order_week']) #TODO not working yet.\n",
    "dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be757018-bbb6-4d00-981b-9973996a5c8a",
   "metadata": {},
   "source": [
    "# Part 1: Exploration, Basic Hypothesis\n",
    "\n",
    "# plot_correlations(\"changes\", \"prev_canceled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a39df9d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#1 number of orders per month. Conclusion: August is the most...\n",
    "\n",
    "# plot_scatter(\"order_year\", \"time_until_order\")\n",
    "orders_per_month = dict(Counter(df['order_month']))\n",
    "months = sorted(list(orders_per_month.keys()))\n",
    "orders_num = [orders_per_month[val] for val in months]\n",
    "plt.barh(orders_num, months)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64dcb7a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#2 number of children affecting month of order\n",
    "df2 = df.groupby(['order_month', 'children']).count()\n",
    "# temo = list(df2.index)\n",
    "mont = [\"\".join(str(i)) for i in list(df2.index)]\n",
    "x = plt.barh(mont, df2['Unnamed: 0'])\n",
    "plt.figure(figsize=(200, 200), dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84405d80-a0da-4ae2-a5fe-cf9b81b8b8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "plot_scatter(\"changes\", \"prev_canceled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab71df3-d1b1-4391-9d10-eeb3e527355a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Notes"
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br>\n",
    "1. People who have deleted their orders more than 20 times.\n",
    "  family with 10 kids that ordered a hotel\n",
    "  -those cases are possible but they are outliers, therefore, we won't remove them\n",
    "<br>\n",
    "2. The data is not scaled.  scaling is essential for machine learning algorithms that calculate distances between data. If not scale, the feature with a higher value range starts dominating when calculating distances.\n",
    "<br>\n",
    "3. There are 253577 missing values in the dataframe. we chose to ignore a sample with missing data because we thought that plugin any value would ruin the plots accuracy.\n",
    "<br>\n",
    "4. The problem has more than 30  features for each row. big dimensionality can affect the predictions' accuracy."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-14-6615347fa781>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;36m  File \u001B[1;32m\"<ipython-input-14-6615347fa781>\"\u001B[1;36m, line \u001B[1;32m1\u001B[0m\n\u001B[1;33m    <br>\u001B[0m\n\u001B[1;37m    ^\u001B[0m\n\u001B[1;31mSyntaxError\u001B[0m\u001B[1;31m:\u001B[0m invalid syntax\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6bc8bb-9aa3-4192-ad04-6f636031a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89926e51-b1c5-4ea0-a90c-06a1cb69c417",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.notnull().sum().sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}